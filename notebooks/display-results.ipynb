{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ea790-0fac-4612-8856-e1242a18510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "chars = ['Relevancy', 'Correctness', 'Completeness', 'Informativeness', 'Integration', 'Cohesion', 'Coherence', 'Readability', 'Conciseness']\n",
    "\n",
    "FOLD_NO = 3\n",
    "\n",
    "def read_json(input_path):\n",
    "    \"\"\"\n",
    "    Reads the ``json`` file of the given ``input_path``.\n",
    "\n",
    "    :param input_path: Path to the json file\n",
    "    :return: A loaded json object.\n",
    "    \"\"\"\n",
    "    with open(input_path, encoding=\"utf-8\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "def word_counter_calculator(column):\n",
    "    N_wc_lower = []\n",
    "    N_wc_upper = []\n",
    "    for col in column:\n",
    "        wc = len(col.strip().split())\n",
    "        N_wc_upper.append(1 if wc > 200 else 0)\n",
    "        N_wc_lower.append(1 if wc < 50 else 0)\n",
    "    return {\"wc_upper\": N_wc_upper, \"wc_lower\": N_wc_lower}\n",
    "\n",
    "def evaluation_report(result_list, model, setting, fold_prefix='gpt-4-eval-'):\n",
    "    print(\"MODEL:\", model)\n",
    "    print(\"SETTING:\", setting)\n",
    "    results_dict = {\n",
    "        \"word-count\":[],\n",
    "        \"WC>200\": [],\n",
    "        \"WC<50\": [],\n",
    "        \"50<=WC<150\": [],\n",
    "        \"150<=WC<=220\": [],\n",
    "        \"220<=WC<=250\": [],\n",
    "        \"150<=WC<=250\": [],\n",
    "        \"WC>250\":[],\n",
    "        \"paper-structure-score\": [],\n",
    "        \n",
    "    }\n",
    "\n",
    "    scores = {\n",
    "        \"Relevancy\":0, \"Correctness\":0, \"Completeness\":0, \n",
    "        \"Informativeness\": 0, \"Integration\": 0, \"Cohesion\":0,\n",
    "        \"Coherence\":0, \"Readability\":0, \"Conciseness\": 0\n",
    "    }\n",
    "    \n",
    "    for inf in result_list:\n",
    "        results_dict['WC>200'].append(inf['basic-eval']['word-count']['WC>200'])\n",
    "        results_dict['WC<50'].append(inf['basic-eval']['word-count']['WC<50'])\n",
    "        results_dict['50<=WC<150'].append(inf['basic-eval']['word-count']['50<=WC<150'])\n",
    "        results_dict['150<=WC<=220'].append(inf['basic-eval']['word-count']['150<=WC<=220'])\n",
    "        results_dict['220<=WC<=250'].append(inf['basic-eval']['word-count']['220<=WC<=250'])\n",
    "        results_dict['150<=WC<=250'].append(inf['basic-eval']['word-count']['150<=WC<=250'])\n",
    "        results_dict['WC>250'].append(inf['basic-eval']['word-count']['WC>250'])\n",
    "        \n",
    "        results_dict['word-count'].append(inf['basic-eval']['word-count']['count'])\n",
    "        results_dict['paper-structure-score'].append(inf['basic-eval']['paper-structure'])\n",
    "\n",
    "        avg_scores = {\n",
    "            \"Relevancy\":0, \"Correctness\":0, \"Completeness\":0, \n",
    "            \"Informativeness\": 0, \"Integration\": 0, \"Cohesion\":0,\n",
    "            \"Coherence\":0, \"Readability\":0, \"Conciseness\": 0\n",
    "        }\n",
    "        for keys in [f'{fold_prefix}{setting}-'+str(idx+1) for idx in range(FOLD_NO)]:\n",
    "            if keys == 'gpt-4-eval-s1-1' and model=='vanila':\n",
    "                items = inf[keys].items()\n",
    "            else:\n",
    "                items = inf[keys]['eval-result'].items()\n",
    "            for key, score in items:\n",
    "                if score['rating'] == '':\n",
    "                    avg_scores[key] += 5\n",
    "                else:\n",
    "                    avg_scores[key] += int(score['rating'])\n",
    "        for key, score in avg_scores.items():\n",
    "            avg_scores[key] = score/FOLD_NO\n",
    "\n",
    "        for key, score in avg_scores.items():\n",
    "            scores[key] += score\n",
    "\n",
    "    for metric, score in scores.items():\n",
    "        scores[metric] /= len(result_list)\n",
    "    \n",
    "    avg = sum(results_dict['WC>200'])/len(results_dict['WC>200'])\n",
    "    print(\"averaged WC>200:\", avg)\n",
    "    \n",
    "    avg_wc = sum(results_dict['word-count'])/len(results_dict['word-count'])\n",
    "    print(\"averaged word counts:\", avg_wc)\n",
    "    \n",
    "    avg = sum(results_dict['150<=WC<=250'])/len(results_dict['150<=WC<=250'])\n",
    "    print(\"averaged 150<=WC<=250:\", avg)\n",
    "    \n",
    "    avg  = sum(results_dict['WC<50'])/len(results_dict['WC<50'])\n",
    "    print(\"averaged WC<50:\", avg)\n",
    "    \n",
    "    avg  = sum(results_dict['50<=WC<150'])/len(results_dict['50<=WC<150'])\n",
    "    print(\"averaged 50<=WC<150:\", avg)\n",
    "    \n",
    "    avg = sum(results_dict['150<=WC<=220'])/len(results_dict['150<=WC<=220'])\n",
    "    print(\"averaged 150<=WC<=220:\", avg)\n",
    "    \n",
    "    avg = sum(results_dict['220<=WC<=250'])/len(results_dict['220<=WC<=250'])\n",
    "    print(\"averaged 220<=WC<=250:\", avg)\n",
    "    \n",
    "    avg = sum(results_dict['WC>250'])/len(results_dict['WC>250'])\n",
    "    print(\"averaged WC>250:\", avg)\n",
    "\n",
    "    print(\"averaged paper-structure:\", sum(results_dict['paper-structure-score'])/len(results_dict['paper-structure-score']))\n",
    "    \n",
    "    return scores, results_dict\n",
    "\n",
    "def get_avg_scores_per_fold(result_list, setting):\n",
    "    avg_scores = {\n",
    "        \"Relevancy\":0, \"Correctness\":0, \"Completeness\":0, \n",
    "        \"Informativeness\": 0, \"Integration\": 0, \"Cohesion\":0,\n",
    "        \"Coherence\":0, \"Readability\":0, \"Conciseness\": 0\n",
    "    }\n",
    "    for inf in result_list:\n",
    "        \n",
    "        try:\n",
    "            items = inf[setting]['eval-result'].items()\n",
    "        except:\n",
    "            items = inf[setting].items()\n",
    "        for key, score in items:\n",
    "            # print (key, score['rating'])\n",
    "            if score['rating'] == '':\n",
    "                avg_scores[key] += 5\n",
    "            else:\n",
    "                avg_scores[key] += int(score['rating'])\n",
    "    for metric, score in avg_scores.items():\n",
    "        avg_scores[metric] /= len(result_list)\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "def build_setting_per_fold_matrix(result_list, setting, chars, fold_prefix='gpt-4-eval-'):\n",
    "    setting_scores = []\n",
    "    for setting_fold in [f'{fold_prefix}{setting}-'+str(idx+1) for idx in range(FOLD_NO)]:\n",
    "        avg = get_avg_scores_per_fold(result_list, setting_fold)\n",
    "        fold_scores = []\n",
    "        for char in chars:\n",
    "            fold_scores.append(avg[char])\n",
    "        setting_scores.append(fold_scores)\n",
    "    return setting_scores\n",
    "\n",
    "def plot_s1_vs_s2(setting1_scores, setting2_scores, model, chars):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    setting1_scores = np.array(setting1_scores)\n",
    "    setting2_scores = np.array(setting2_scores)\n",
    "\n",
    "    ig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "    # Scatter plot for Setting 1\n",
    "    for i, char in enumerate(chars):\n",
    "        ax.scatter(np.full_like(setting1_scores[:, i], i*2 - 0.2), setting1_scores[:, i], color='blue', alpha=0.7)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        ax.scatter(np.full_like(np.mean(setting1_scores[:, i]), i*2 - 0.2), np.mean(setting1_scores[:, i]), color='green', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Scatter plot for Setting 2\n",
    "    for i, char in enumerate(chars):\n",
    "        ax.scatter(np.full_like(setting2_scores[:, i], i*2 + 0.2), setting2_scores[:, i],  color='red', alpha=0.7)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        ax.scatter(np.full_like(np.mean(setting2_scores[:, i]), i*2 + 0.2), np.mean(setting2_scores[:, i]), color='black', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Create custom legend handles\n",
    "    blue_dot = plt.Line2D([0], [0], marker='o', color='blue', linestyle='None', markersize=5)\n",
    "    red_dot = plt.Line2D([0], [0], marker='o', color='red', linestyle='None', markersize=5)\n",
    "    green_dot = plt.Line2D([0], [0], marker='o', color='green', linestyle='None', markersize=5)\n",
    "    black_dot = plt.Line2D([0], [0], marker='o', color='black', linestyle='None', markersize=5)\n",
    "\n",
    "    # Customizing the plot\n",
    "    ax.set_xticks(np.arange(len(chars)) * 2)\n",
    "    ax.set_xticklabels(chars)\n",
    "    ax.set_xlabel('Characteristics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    # ax.set_title('Scatter Plot of Scores by Characteristics and Evaluation Settings')\n",
    "\n",
    "    legend_labels = ['Setting-1', 'Setting-2', \"Setting-1 Avg. Scores\", \"Setting-2 Avg. Scores\"]\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(handles=[blue_dot, red_dot, green_dot, black_dot], labels=legend_labels, loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"images/{model}-s1-vs-s2.jpg\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_s1_vs_s2_vs_prolofic(setting1_scores, setting2_scores, prolofic_scores, model, chars):\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    setting1_scores = np.array(setting1_scores)\n",
    "    setting2_scores = np.array(setting2_scores)\n",
    "    prolofic_scores =  np.array(prolofic_scores)\n",
    "\n",
    "    ig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "    # Scatter plot for Setting 1\n",
    "    # for i, char in enumerate(chars):\n",
    "    #     ax.scatter(np.full_like(setting1_scores[:, i], i*2 - 0.2), setting1_scores[:, i], color='blue', alpha=0.7)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        ax.scatter(np.full_like(np.mean(setting1_scores[:, i]), i*2 - 0.2), np.mean(setting1_scores[:, i]), color='blue', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Scatter plot for Setting 2\n",
    "    # for i, char in enumerate(chars):\n",
    "    #     ax.scatter(np.full_like(setting2_scores[:, i], i*2 + 0.2), setting2_scores[:, i],  color='red', alpha=0.7)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        ax.scatter(np.full_like(np.mean(setting2_scores[:, i]), i*2 + 0.2), np.mean(setting2_scores[:, i]), color='black', alpha=0.7)\n",
    "    \n",
    "    # Scatter plot for prolofic_scores 2\n",
    "    for i, char in enumerate(chars):\n",
    "        ax.scatter(np.full_like(np.mean(prolofic_scores[:, i]), i*2 + 0.2), np.mean(prolofic_scores[:, i]), color='red', alpha=0.7)\n",
    "\n",
    "    # Create custom legend handles\n",
    "    blue_dot = plt.Line2D([0], [0], marker='o', color='blue', linestyle='None', markersize=5)\n",
    "    red_dot = plt.Line2D([0], [0], marker='o', color='red', linestyle='None', markersize=5)\n",
    "    green_dot = plt.Line2D([0], [0], marker='o', color='green', linestyle='None', markersize=5)\n",
    "    black_dot = plt.Line2D([0], [0], marker='o', color='black', linestyle='None', markersize=5)\n",
    "\n",
    "    # Customizing the plot\n",
    "    ax.set_xticks(np.arange(len(chars)) * 2)\n",
    "    ax.set_xticklabels(chars)\n",
    "    ax.set_xlabel('Characteristics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    # ax.set_title('Scatter Plot of Scores by Characteristics and Evaluation Settings')\n",
    "\n",
    "    legend_labels = [\"Setting-1 Avg. Scores\", \"Setting-2 Avg. Scores\", \"Prolofic-Human Avg. Scores\" ]\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(handles=[blue_dot, black_dot, red_dot], labels=legend_labels, loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"images/{model}-s1-vs-s2-vs-prolofic.jpg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8d2bc-a6cd-497f-a264-8d388e0e8e23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Prolofic Human Evaluation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68aba6-0eea-46bb-9537-190b209a2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scripts.dataset import format_context\n",
    "from scripts.utils import paper_structure_score, word_count_score\n",
    "from scripts.configs import BaseConfig\n",
    "from scripts import io\n",
    "\n",
    "args = BaseConfig().get_args()\n",
    "\n",
    "reward_vocab = io.read_text(args.reward_vocab).split(\"\\n\")\n",
    "reward_vocab = [vocab.lower() for vocab in reward_vocab]\n",
    "    \n",
    "mdf = pd.read_csv(\"dataset/split/Mistral-7B/ProlificHuman/prolific_mistral_methodological.csv\")\n",
    "pdf = pd.read_csv(\"dataset/split/Mistral-7B/ProlificHuman/prolific_mistral_paper-wise.csv\")\n",
    "tdf = pd.read_csv(\"dataset/split/Mistral-7B/ProlificHuman/prolific_mistral_thematic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4b412-59d0-46de-9ad6-99a22485f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prolofic_dataset = []\n",
    "\n",
    "\n",
    "def format_context(row):\n",
    "    context = \"\"\n",
    "    for i in range(5):\n",
    "        title = row[f'paper_{i+1}_title'] \n",
    "        abstract = row[f'paper_{i+1}_abstract'] \n",
    "        context += f'{i+1}. ' + ' '.join(title.replace('\\n', ' ').split()) + '\\n' + ' '.join(abstract.replace('\\n', ' ').split()) + '\\n\\n'\n",
    "    return context \n",
    "\n",
    "synthesis_types = [\"methodological\", \"paperwise\", \"thematic\"]\n",
    "\n",
    "ph_dfs = [mdf, pdf, tdf]\n",
    "for idx, synthesis_type in enumerate(synthesis_types):\n",
    "    \n",
    "    for index, row in ph_dfs[idx].iterrows(): \n",
    "        context = format_context(row)\n",
    "        research_problem = row['research_problem']\n",
    "        eval_results = {}\n",
    "        for person in [\"P1\", \"P2\", \"P3\"]:\n",
    "            eval_result = {}\n",
    "            for char in chars:\n",
    "                char_lower = char.lower()\n",
    "                rating = row[f'{char_lower}_rating_{person}']\n",
    "                rationale = row[f'{char_lower}_comment_{person}']\n",
    "                eval_result[char] = {\"rating\":rating, \"rationale\":rationale}\n",
    "            eval_results[person] = eval_result\n",
    "\n",
    "\n",
    "        prolofic_dataset.append({\n",
    "            \"synthesis\": synthesis_type, \n",
    "            \"split\": 'test-prolific', \n",
    "            \"sample_id\":row['sample_id'], \n",
    "            \"prompt\": \"\", \n",
    "            \"inference\": row['synthesis_text'],\n",
    "            \"P-1\": {\"eval-result\": eval_results['P1']},\n",
    "            \"P-2\": {\"eval-result\": eval_results['P2']},\n",
    "            \"P-3\": {\"eval-result\": eval_results['P3']},\n",
    "            \"basic-eval\": {\n",
    "                    \"paper-structure\": paper_structure_score(row['synthesis_text'], reward_vocab), \n",
    "                    \"word-count\": word_count_score(row['synthesis_text'])\n",
    "            }\n",
    "        })\n",
    "\n",
    "len(prolofic_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e388c6-b56c-4b54-930c-4f8bdd9830b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prolofic_dataset_score, prolofic_dataset_reults_dict =  evaluation_report(prolofic_dataset, model='prolofic_dataset', setting='P', fold_prefix='')\n",
    "print(json.dumps(prolofic_dataset_score, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "vanila = read_json(\"assets/vanila.json\")\n",
    "vanila_prolific = [item for item in vanila if item['split'] == 'test-prolific']\n",
    "\n",
    "vanila_prolific_score_s1, vanila_prolific_reults_dict_s1 =  evaluation_report(vanila_prolific, model='vanila', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(vanila_prolific_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "vanila_prolific_score_s2, vanila_prolific_reults_dict_s2 =  evaluation_report(vanila_prolific, model='vanila', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(vanila_prolific_score_s2, indent=4))\n",
    "\n",
    "setting1_scores_vanila_prolific = build_setting_per_fold_matrix(result_list=vanila_prolific, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "setting2_scores_vanila_prolific = build_setting_per_fold_matrix(result_list=vanila_prolific, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "prolific_scores_vanila_prolific = build_setting_per_fold_matrix(result_list=prolofic_dataset, setting='P', chars=chars, fold_prefix='')\n",
    "\n",
    "plot_s1_vs_s2_vs_prolofic(setting1_scores=setting1_scores_vanila_prolific, \n",
    "                          setting2_scores=setting2_scores_vanila_prolific, \n",
    "                          prolofic_scores=prolific_scores_vanila_prolific, \n",
    "                          model='vanila-prolific', chars=chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc532b7-567d-4972-a62e-8441f71658b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prolofic_dataset_score, prolofic_dataset_reults_dict =  evaluation_report(prolofic_dataset, model='prolofic_dataset', setting='P', fold_prefix='')\n",
    "print(json.dumps(prolofic_dataset_score, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "warmup = read_json(\"assets/warmup-inf.json\")\n",
    "warmup_prolific = [item for item in warmup if item['split'] == 'test-prolific']\n",
    "\n",
    "warmup_prolific_score_s1, warmup_prolific_reults_dict_s1 =  evaluation_report(warmup_prolific, model='warmup', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(warmup_prolific_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "warmup_prolific_score_s2, warmup_prolific_reults_dict_s2 =  evaluation_report(warmup_prolific, model='warmup', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(warmup_prolific_score_s2, indent=4))\n",
    "\n",
    "setting1_scores_warmup_prolific = build_setting_per_fold_matrix(result_list=warmup_prolific, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "setting2_scores_warmup_prolific = build_setting_per_fold_matrix(result_list=warmup_prolific, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "prolific_scores_warmup_prolific = build_setting_per_fold_matrix(result_list=prolofic_dataset, setting='P', chars=chars, fold_prefix='')\n",
    "\n",
    "plot_s1_vs_s2_vs_prolofic(setting1_scores=setting1_scores_warmup_prolific, \n",
    "                          setting2_scores=setting2_scores_warmup_prolific, \n",
    "                          prolofic_scores=prolific_scores_warmup_prolific, \n",
    "                          model='finetuned-prolific', chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd6e773-e7ac-4cec-ac8b-10740858e294",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Vanila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939747be-6f5d-4610-8def-2afca3d7086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanila = read_json(\"assets/vanila.json\")\n",
    "vanila_score_s1, vanila_reults_dict_s1 =  evaluation_report(vanila, model='vanila', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(vanila_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "vanila_score_s2, vanila_reults_dict_s2 =  evaluation_report(vanila, model='vanila', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(vanila_score_s2, indent=4))\n",
    "\n",
    "setting1_scores_vanila = build_setting_per_fold_matrix(result_list=vanila, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "setting2_scores_vanila = build_setting_per_fold_matrix(result_list=vanila, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "plot_s1_vs_s2(setting1_scores=setting1_scores_vanila, setting2_scores=setting2_scores_vanila, model='vanila', chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82046ad-4e4e-4fed-a7a0-cb6562443e70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b1f3d-92c7-4f05-82fb-cd85ee9cfaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup = read_json(\"assets/warmup-inf.json\")\n",
    "warmup_score_s1, warmup_reults_dict_s1 =  evaluation_report(warmup, model='warmup', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(warmup_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "warmup_score_s2, warmup_reults_dict_s2 =  evaluation_report(warmup, model='warmup', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(warmup_score_s2, indent=4))\n",
    "\n",
    "setting1_scores_warmup = build_setting_per_fold_matrix(result_list=warmup, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')        \n",
    "setting2_scores_warmup = build_setting_per_fold_matrix(result_list=warmup, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "plot_s1_vs_s2(setting1_scores=setting1_scores_warmup, setting2_scores=setting2_scores_warmup, model='warmup', chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66150f1-0856-467a-b5ef-b0f3cf2130e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Finetuned + RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a275a3339b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_rlhf = read_json(\"assets/rlhf-style-with-warmup-inf.json\")\n",
    "finetuned_rlhf_score_s1, finetuned_rlhf_reults_dict_s1 =  evaluation_report(finetuned_rlhf, model='finetuned_rlhf', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(finetuned_rlhf_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "finetuned_rlhf_score_s2, finetuned_rlhf_reults_dict_s2 =  evaluation_report(finetuned_rlhf, model='finetuned_rlhf', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(finetuned_rlhf_score_s2, indent=4))\n",
    "\n",
    "setting1_scores_finetuned_rlhf = build_setting_per_fold_matrix(result_list=finetuned_rlhf, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')        \n",
    "setting2_scores_finetuned_rlhf = build_setting_per_fold_matrix(result_list=finetuned_rlhf, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "plot_s1_vs_s2(setting1_scores=setting1_scores_finetuned_rlhf, setting2_scores=setting2_scores_finetuned_rlhf, model='finetuned_rlhf', chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cfacc-7155-4ab6-bcc6-5de7aa6d8aed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399ac65-697f-4ee6-9c3a-588f04f95c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf = read_json('assets/rlhf-style-inf.json')\n",
    "rlhf_score_s1, rlhf_reults_dict_s1 =  evaluation_report(rlhf, model='rlhf', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(rlhf_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "rlhf_score_s2, rlhf_reults_dict_s2 =  evaluation_report(rlhf, model='rlhf', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(rlhf_score_s2, indent=4))\n",
    "\n",
    "setting1_scores_rlhf = build_setting_per_fold_matrix(result_list=rlhf, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')        \n",
    "setting2_scores_rlhf = build_setting_per_fold_matrix(result_list=rlhf, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "plot_s1_vs_s2(setting1_scores=setting1_scores_rlhf, setting2_scores=setting2_scores_rlhf, model='rlhf', chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdcce8c-47d0-47a2-9319-25de36eabb14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Finetuned + RLHF (GPT4-Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65621b9a-e4e8-43ef-99b5-8560c411dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf = read_json('assets/rlhf-style-gpt4-with-warmup-inf.json')\n",
    "rlhf_score_s1, rlhf_reults_dict_s1 =  evaluation_report(rlhf, model='finetuned_rlhf_gpt4', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(rlhf_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "rlhf_score_s2, rlhf_reults_dict_s2 =  evaluation_report(rlhf, model='finetuned_rlhf_gpt4', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(rlhf_score_s2, indent=4))\n",
    "\n",
    "setting1_scores_rlhf = build_setting_per_fold_matrix(result_list=rlhf, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')        \n",
    "setting2_scores_rlhf = build_setting_per_fold_matrix(result_list=rlhf, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "plot_s1_vs_s2(setting1_scores=setting1_scores_rlhf, setting2_scores=setting2_scores_rlhf, model='finetuned_rlhf_gpt4', chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2e9e5-5ae2-4ac9-a82b-e2238bae011d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# RLHF (GPT4-Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd02163-8e65-4e31-b1e1-aa3d45288779",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf = read_json('assets/rlhf-style-gpt4-inf.json')\n",
    "rlhf_score_s1, rlhf_reults_dict_s1 =  evaluation_report(rlhf, model='rlhf_gpt4', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(rlhf_score_s1, indent=4))\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "# rlhf_score_s2, rlhf_reults_dict_s2 =  evaluation_report(rlhf, model='rlhf_gpt4', setting='s2', fold_prefix='gpt-4-eval-')\n",
    "# print(json.dumps(rlhf_score_s2, indent=4))\n",
    "\n",
    "# setting1_scores_rlhf = build_setting_per_fold_matrix(result_list=rlhf, setting='s1', chars=chars, fold_prefix='gpt-4-eval-')        \n",
    "# setting2_scores_rlhf = build_setting_per_fold_matrix(result_list=rlhf, setting='s2', chars=chars, fold_prefix='gpt-4-eval-')\n",
    "# plot_s1_vs_s2(setting1_scores=setting1_scores_rlhf, setting2_scores=setting2_scores_rlhf, model='rlhf_gpt4', chars=chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae05f88-6aac-4b88-9507-94b2a04b4fef",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPT-4 Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb4361-4945-499a-ad2f-8df04ebdb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = read_json('assets/gpt4-vanilla.json')\n",
    "rlhf_score_s1, rlhf_reults_dict_s1 =  evaluation_report(result_list, model='gpt4_vanila', setting='s1', fold_prefix='gpt-4-eval-')\n",
    "print(json.dumps(rlhf_score_s1, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ffa80-d8ec-45be-9222-c66a2f9ad465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
